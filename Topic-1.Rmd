## Topic 1 Exercises
# Assignment 1 â€” file Topic-1.Rmd
# Qinxi Wang
## Due Sep 20

```
    Discussion questions: ISL 2.4.1, 2.4.3, 2.4.6
    Computing assignment: ISL 2.4.8, 2.4.9
    Theory assignment: ISL 2.4.2, 2.4.7.
```

### Discussion Question

##### 2.4.1
```
(a) In the case of large sample size, we want to use a flexible method to include as much training data as possible, and the flexible method allows us to do that while less likely to overfitting. Since the number of predictors are also small, a more flexbile method would help reduce the bias as well.

(b) As opposite to part (a), in the case of a small sample size we prefer a inflexible method to avoid overfitting. Also given the large number of predictors, using a flexible method would cause a big inflation in Y if change any X, and would cause a higher variance. Thus inflexbile method would outperform it.

(c) Since its already indicated that the relationship between the predictors and Y is highly non-linear, a flexible method does a better job in fitting such non-linear relationship.

(d) We would prefer a inflexible method since if we change any training data, it would cause the estimated value to change considerably given the large variance of the error. Thus in order to aviod overfitting, a inflexible method does a better job.
```


#### 2.4.3
```
(a) 
```
<div id="bg">
  <img src="Q2.png" alt="">
</div>
```

(b). 
(i). Bias(Squared) is the error introduced by using a simple model to approximate a real-life problem. The error term is reduced as model flexibility increases because we would be able to fit the data more closely.

(ii). Variance is the amount by which f-hat would change if we estimate it using a different traing dataset. When an overly flexible model is used to learn from data, hence fitting the noise too closely and fails to generalize to out-of-sample test data. If general, as we use more flexible methods, variancegoes up as bias decrease.

(iii). Training error is the amount of response of a training sample equals the training response. It will keep decreasing as model flexibility increases, because as the model become more flexible it will fit all data points more and more closely.

(iv). Test error is the amount to which the response of a test sample equals the testing response. As flexibility increases, the test error will decrease till a certain extent until the model becomes overfitting, then it will climb again.

(v). Bayes is the upper-bound for the accuracy of any given model, it is the lowest test error rate possible. Since its the max, the value remain as a constant.
```

#### 2.4.6
```
Parametric method is to make an assumption about the function f, and then fit or train the model. While Non-parametric approach is when there is no explicit assumption about the function, but seek estimate of f that gets as close to the data points as possible. 

The advantage of parametric approach comes in as that it doesn't require as much data since the parameters are sort of given, also it might give a better prediction/fit a winder range of test data since its easier to avoid overfitting in this case. While for Non-parametric method, not only the computation required to analyze each data point is more than parametric one, the danger of overfitting is also higher since we have no assumption about how the function f looks like.

The disadvantages of a parametric approach, then, might cause due to an inaccurate estimate f. Since if we already made our assumption about f wrong in the first place, the form of f assumed is highly likely to overfit the observations if more flexible models are used.
```


### Computation Assignment

#### 2.4.8
##### (a)
```{r}
library("ISLR")
#download.file("http://www-bcf.usc.edu/~gareth/ISL/College.csv",destfile="College.csv")
data(College)
```

##### (b)
```
fix() returns error in .External2(C_dataentry, datalist, modes), so commented out
```
```{r}
#rownames = College[, 1]
#College = College[, -1]
#fix(College)
```

##### (c)

```{r}
summary(College)
pairs(College[, 1:10])

plot(College$Private, College$Outstate)
```

```{r}
Elite <- rep("No", nrow(College))
Elite[College$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(College, Elite)

summary(Elite)
plot(Elite, College$Outstate)
```

```{r}
par(mfrow = c(2,2))
names(college)
hist(college$Apps, col = 2)
hist(college$Enroll, col = 3)
hist(college$S.F.Ratio, col = 4)
hist(college$Terminal, col = 5)
```


#### 2.4.9
```{r}
#download.file("http://www-bcf.usc.edu/~gareth/ISL/Auto.csv",destfile="Auto.csv")
auto_file_name = "/home/local/MAC/qwang/Auto.csv"
Auto = read.csv(auto_file_name)
```
(a)
```
Quantitative variables - mpg, displacement, horsepower, weight, acceleration, cylinders
Qualitative variables - year, origin
```

(b)
```{r}
summary(Auto[, -c(4, 9)])
```

(c)
```{r}
sapply(Auto[, -c(4, 9)], mean)
sapply(Auto[, -c(4, 9)], sd)
```

(d)
```{r}
subset = Auto[-c(10:85), -c(4,9)]
sapply(subset, range)
sapply(subset, mean)
sapply(subset, sd)
```

(e)
```{r}
pairs(Auto)
names(Auto)
```

(f)
```
Yes. Such that gas mileage mpg can be predicted using other variables listed above in the table, and there are about 3-4 predicots used given the graph.
```


### Theory Assignment

#### 2.4.2
```
(a) Regression. Since we're using the set of predictor data in order to understand the relation between (the most significant) preditor and the response, we are using inference. 
Finally, n = sample size = 500, p = num of predictors = 3

(b) Classification since the output we're looking for are qualitative. And since we are using the previous data on other product to predict it, we are using predication.
n=20 and p=13

(c) Prediction as indicated in the question already, since we used past data in order to changing understand the market change, we're using Regression as well.
n= 52 * num of weekly data of changes and p=4 - all the affected variable.
```

#### 2.4.7

(a) 
```{r}
distance=c(3,2,sqrt(10), sqrt(5),sqrt(2),sqrt(3))
(distance)
```

```
(b) Green. Since we want to find the case such that the nearest neighbor for K = 1, we notice in observation 5 the node has 1 nearest neighbor.

(c) Red. Same reasoning as above, the event happending with a Red is 2/3 while comparing to Green's 1/3.

(d) If Bayes decision bound in this case is highly non-linear, it means that a small K would give a more flexible model to fit a non-inear decision boundary. As K becomes larger, the boundary becomes less flexible, thus we expect the best value for K, which is the upper bound for fitting a linear/very inflexible model, the value of K should be small.
```


